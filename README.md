# AdamW-mini-SF
Lightweight optimizer with ScheduleFree &amp; AMP support

**Lightweight, schedule-free optimizer based on AdamW â€” with automatic learning rate adjustment and AMP support.**

This optimizer extends [Adam-mini](https://github.com/zyushun/Adam-mini) with:

- ğŸš€ **Memory-efficient state**: keeps moments (`m`, `v`) in low-precision (e.g., `float16` / `bfloat16`)
- ğŸ§  **Schedule-Free learning rate adaptation**: adjusts `lr` dynamically using smoothed gradient norms (no schedulers needed)
- ğŸ›¡ï¸ **Decoupled weight decay**: follows AdamW-style decay, separate from gradients
- âš™ï¸ **AMP/mixed-precision support**: detects parameter dtypes for seamless integration with `torch.amp` or custom precision

## Installation

Simply copy the `adamw_mini_sf.py` file into your project.

## Usage

```python
from adamw_mini_sf import AdamWminiScheduleFree
optimizer = AdamWminiScheduleFree(model.parameters(), lr=1e-3)
```
If dtype is omitted, the optimizer will follow p.data.dtype to determine the internal precision. However, to enable half precision (for memory savings), it must be explicitly specified:
```python
optimizer = AdamWminiScheduleFree(model.parameters(), lr=1e-3, dtype=torch.float16)
```
With this, optimizer states like exp_avg and exp_avg_sq will be stored in half precision, allowing for both memory and performance optimizations.

License
Apache License 2.0 â€” see LICENSE for details.

Built with ğŸ¤– GitHub Copilot + human curiosity.

## Acknowledgments

This project builds upon the excellent work of [Adam-mini](https://github.com/zyushun/Adam-mini) by @zyushun â€” thank you for your contributions to lightweight optimizers.

Thanks also to the open-source community behind PyTorch, and to GitHub Copilot for being an inspiring coding partner.

We are grateful to the research community whose ideas around AdamW, Schedule-Free optimization, and mixed precision have made this possible.
 
![AdamW-mini-ScheduleFree00](https://github.com/muooon/adamw-mini-ScheduleFree/blob/main/AdamW-mini-ScheduleFree00.png?raw=true)
The test code is provided at the end.
 
# AdamW-mini-SF

**AdamW ã«åŸºã¥ã„ãŸè»½é‡ã‹ã¤ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ä¸è¦ãªæœ€é©åŒ–æ‰‹æ³• â€” è‡ªå‹•å­¦ç¿’ç‡èª¿æ•´ï¼†AMPã‚µãƒãƒ¼ãƒˆå¯¾å¿œã€‚**

ã“ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¯ã€[Adam-mini](https://github.com/zyushun/Adam-mini) ã‚’æ‹¡å¼µã—ã€ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¡ã¾ã™ï¼š

- ğŸš€ **çœãƒ¡ãƒ¢ãƒªãªçŠ¶æ…‹ç®¡ç†**ï¼šãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ(`m`, `v`)ã‚’ä½ç²¾åº¦(`float16` ã‚„ `bfloat16`)ã§ä¿æŒ
- ğŸ§  **Schedule-Free ãªå­¦ç¿’ç‡èª¿æ•´**ï¼šã‚¹ãƒ ãƒ¼ã‚ºãªå‹¾é…ãƒãƒ«ãƒ ã‚’è¿½è·¡ã—ã€`lr` ã‚’å‹•çš„ã«èª¿æ•´(ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ä¸è¦)
- ğŸ›¡ï¸ **åˆ†é›¢ã•ã‚ŒãŸWeight Decay(AdamWå½¢å¼)**ï¼šå‹¾é…ã¨ã¯ç‹¬ç«‹ã—ãŸæ­£å‰‡åŒ–å‡¦ç†
- âš™ï¸ **AMP / mixed precision ã«å¯¾å¿œ**ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® dtype ã‚’è‡ªå‹•æ¤œå‡ºã—ã€`torch.amp` ã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«é€£æºå¯èƒ½

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

`adamw_mini_sf.py` ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚

## ä½¿ã„æ–¹

```python
from adamw_mini_sf import AdamWminiScheduleFree
optimizer = AdamWminiScheduleFree(model.parameters(), lr=1e-3)
```
dtype ã‚’çœç•¥ã™ã‚Œã° p.data.dtype ã«å¾“ã£ã¦çŠ¶æ…‹ãŒä½œã‚‰ã‚Œã¾ã™ãŒã€åŠç²¾åº¦ï¼ˆçœãƒ¡ãƒ¢ãƒªåŒ–ï¼‰ã‚’æœ‰åŠ¹ã«ã—ãŸã„å ´åˆã¯æ˜ç¤ºãŒå¿…è¦ã§ã™ï¼š
```python
optimizer = AdamWminiScheduleFree(model.parameters(), lr=1e-3, dtype=torch.float16)
```
ã“ã‚Œã§ exp_avg / exp_avg_sq ãªã©ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãŒåŠç²¾åº¦ã§ä¿æŒã•ã‚Œã€ãƒ¡ãƒ¢ãƒªãƒ»é€Ÿåº¦ã®æœ€é©åŒ–ãŒåŠ¹ãã¾ã™ã€‚

ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
Apache License 2.0 â€” è©³ç´°ã¯ LICENSE ã‚’ã”è¦§ãã ã•ã„ã€‚

ğŸ¤– GitHub Copilot ã¨äººé–“ã®å¥½å¥‡å¿ƒã®ã‚³ãƒ©ãƒœã§èª•ç”Ÿã—ã¾ã—ãŸã€‚

## è¬è¾(Acknowledgments)

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€[@zyushun](https://github.com/zyushun) æ°ã«ã‚ˆã‚‹ [Adam-mini](https://github.com/zyushun/Adam-mini) ã®ç´ æ™´ã‚‰ã—ã„å…ˆè¡Œç ”ç©¶ã¨å®Ÿè£…ã«å¤šãã‚’å­¦ã³ã€ãã®ä¸Šã«æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚è»½é‡ã‹ã¤é«˜æ€§èƒ½ãªæœ€é©åŒ–å™¨ã®ç¤ã‚’ç¯‰ã„ã¦ã„ãŸã ãã€æ·±ãæ„Ÿè¬ç”³ã—ä¸Šã’ã¾ã™ã€‚

ã¾ãŸã€PyTorch ãŠã‚ˆã³ OSS ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®çš†ã•ã¾ã€Schedule-Free æœ€é©åŒ–ã‚„ mixed precision å­¦ç¿’ã«é–¢ã™ã‚‹ç ”ç©¶ã‚’ç¯‰ã„ã¦ããŸç ”ç©¶è€…ã®æ–¹ã€…ã®çŸ¥è¦‹ã«ã€å¿ƒã‚ˆã‚Šæ•¬æ„ã‚’è¡¨ã—ã¾ã™ã€‚

ã•ã‚‰ã«ã€æœ¬å®Ÿè£…ã«ã‚ãŸã£ã¦ã¯ GitHub Copilot ã¨ã®å”åƒã‚‚å¤§ããªåŠ©ã‘ã¨ãªã‚Šã¾ã—ãŸã€‚AIæ”¯æ´ã«ã‚ˆã‚‹é–‹ç™ºã®å¯èƒ½æ€§ã«æ„Ÿè¬ã™ã‚‹ã¨ã¨ã‚‚ã«ã€ã“ã‚Œã‹ã‚‰ã‚‚äººé–“ã¨AIã®å…±å‰µãŒåºƒãŒã‚‹ã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚

## Benchmark Code (for Reproducibility)
### æ¯”è¼ƒå®Ÿé¨“ã‚³ãƒ¼ãƒ‰ï¼ˆå†ç¾ç”¨ï¼‰

Below is a test script that compares the processing speed and memory usage of AdamW and AdamW-mini-ScheduleFree. You can copy and run it as-is to reproduce the results.

ä»¥ä¸‹ã¯ã€AdamWã¨AdamW-mini-ScheduleFreeã®å‡¦ç†é€Ÿåº¦ãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¯”è¼ƒã—ãŸãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã§ã™ã€‚å†ç¾æ€§ã®ãŸã‚ã€ãã®ã¾ã¾è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã§ãã¾ã™ã€‚

<details>
<summary>Show Test Code | ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º</summary>

```python
import torch, time
import matplotlib.pyplot as plt
from torch import nn, utils
from torch.optim import AdamW
from torch.utils.checkpoint import checkpoint_sequential

from adamw_mini_sf import AdamWminiScheduleFree

import matplotlib
matplotlib.rcParams['font.family'] = 'Meiryo'  # Windowsã®å ´åˆ

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆ3ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ã¦checkpointingå¯¾å¿œï¼‰
class CheckpointedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, 2048)
        )
    def forward(self, x):
        return checkpoint_sequential(self.seq, 3, x, use_reentrant=False)


# åˆæœŸåŒ–ï¼†fp16åŒ–
model = CheckpointedModel().cuda() #.half()

# ãƒ‡ãƒ¼ã‚¿ã‚‚fp16
x = torch.randn(16, 2048, dtype=torch.float16, device='cuda', requires_grad=True)
y = torch.randn(16, 2048, dtype=torch.float16, device='cuda')
loss_fn = nn.MSELoss()

optimizers = {
    "AdamW": lambda: AdamW(model.parameters(), lr=1e-3),
    "AdamW-mini-SF": lambda: AdamWminiScheduleFree(model.parameters(), lr=1e-3, dtype=torch.float16)
}

records = {}
for name, opt_fn in optimizers.items():
    torch.cuda.empty_cache()
    torch.manual_seed(42)
    model.apply(lambda m: hasattr(m, "reset_parameters") and m.reset_parameters())

    mem_log, time_log = [], []
    optimizer = opt_fn()
    scaler = torch.cuda.amp.GradScaler()  # AMPã¨ä½µç”¨å¯

    for _ in range(50):
        torch.cuda.synchronize()
        t0 = time.perf_counter()

        with torch.autocast(device_type='cuda', dtype=torch.float16):
            y_pred = model(x)
            loss = loss_fn(y_pred, y)

        optimizer.zero_grad(set_to_none=True)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        torch.cuda.synchronize()
        t1 = time.perf_counter()

        mem_mb = torch.cuda.memory_allocated() / 1024**2
        mem_log.append(mem_mb)
        time_log.append((t1 - t0) * 1000)

    records[name] = {"mem": mem_log, "time": time_log}

# ã‚°ãƒ©ãƒ•æç”»
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
for name, data in records.items():
    plt.plot(data["mem"], label=name)
plt.ylabel("VRAMä½¿ç”¨é‡ (MB)")
plt.xlabel("Iteration")
plt.title("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ")
plt.legend()

plt.subplot(1, 2, 2)
for name, data in records.items():
    plt.plot(data["time"], label=name)
plt.ylabel("1ã‚¹ãƒ†ãƒƒãƒ—æ™‚é–“ (ms)")
plt.xlabel("Iteration")
plt.title("å‡¦ç†æ™‚é–“ã®æ¯”è¼ƒ")
plt.legend()

plt.tight_layout()
plt.show()
```